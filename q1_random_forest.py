# -*- coding: utf-8 -*-
"""q1_random_forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m0lDK_7a1_66zC8GxeLKRiJ_awY4yyyJ
"""

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/connect-4/connect-4.data.Z
!uncompress connect-4.data.Z

import pandas as pd
from sklearn.model_selection import GridSearchCV, cross_val_score
from scipy.stats import randint
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
import numpy as np


# Define mapping for 'b', 'o', and 'x'
mapping = {'b': 0, 'o': 1, 'x': 2}

# Read dataset and replace values with integers
df = pd.read_csv('connect-4.data', header=None)
df.replace(mapping, inplace=True)

# Define mapping for 'won', 'loss', and 'draw'
outcome_mapping = {'win': 1, 'loss': 0, 'draw': 2}

# Replace outcome values with integers
df.replace(outcome_mapping, inplace=True)



#df = pd.read_csv('connect-4.data', header=None)



# Separate the target variable from the rest of the data
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

#X=pd.get_dummies(X)
#y=pd.get_dummies(y)

# Split the data into training and testing sets


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



# Create a random forest classifier with default parameters
print("_RF on gini_")
rf = RandomForestClassifier(criterion='gini',random_state=42)
rf.fit(X_train, y_train)
y_pred=rf.predict(X_test)
print("Accuracy_model_on_test:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1-score:", f1_score(y_test, y_pred, average='macro'))

# Evaluate the classifier using 10-fold cross validation
scores = cross_val_score(rf, X_train, y_train, cv=10)
print("Average_accuracy_cross_value: {:.3f}".format(np.mean(scores)))

# Define the parameter grid
param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 10, 20],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}
# Create a grid search object
grid_search = GridSearchCV(rf, param_grid, cv=5)
# Fit the grid search object to the data
grid_search.fit(X_train, y_train)
# Print the best parameters and score
print("Best parameters: {}".format(grid_search.best_params_))
print("Best score: {:.3f}".format(grid_search.best_score_))
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print("Accuracy_grid:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1-score:", f1_score(y_test, y_pred, average='macro'))

# Define the parameter distribution
param_dist = {
    "n_estimators": sp_randint(50, 200),
    "max_depth": [None, 10, 20],
    "min_samples_split": sp_randint(2, 10),
    "min_samples_leaf": sp_randint(1, 4),
    "bootstrap": [True, False],
    "criterion": ["gini", "entropy"]
}
# Create a randomized search object
random_search = RandomizedSearchCV(
    rf, param_distributions=param_dist, n_iter=50, cv=5, random_state=42)
random_search.fit(X_train, y_train)
print(f"Best parameters: {random_search.best_params_}")
print(f"Accuracy_rnd_param: {random_search.best_score_:.2f}")
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
print("Accuracy_rnd_param_on_test:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1-score:", f1_score(y_test, y_pred, average='macro'))



print("_RF on entropy_")
rf = RandomForestClassifier(criterion='entropy',random_state=42)
rf.fit(X_train, y_train)
y_pred=rf.predict(X_test)
print("Accuracy_model_on_test:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1-score:", f1_score(y_test, y_pred, average='macro'))

# Evaluate the classifier using 10-fold cross validation
scores = cross_val_score(rf, X_train, y_train, cv=10)
print("Average_accuracy_cross_value: {:.3f}".format(np.mean(scores)))

# Define the parameter grid
param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 10, 20],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}
# Create a grid search object
grid_search = GridSearchCV(rf, param_grid, cv=5)
# Fit the grid search object to the data
grid_search.fit(X_train, y_train)
# Print the best parameters and score
print("Best parameters: {}".format(grid_search.best_params_))
print("Best score: {:.3f}".format(grid_search.best_score_))
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print("Accuracy_grid:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1-score:", f1_score(y_test, y_pred, average='macro'))

# Define the parameter distribution
param_dist = {
    "n_estimators": sp_randint(50, 200),
    "max_depth": [None, 10, 20],
    "min_samples_split": sp_randint(2, 10),
    "min_samples_leaf": sp_randint(1, 4),
    "bootstrap": [True, False],
    "criterion": ["gini", "entropy"]
}
# Create a randomized search object
random_search = RandomizedSearchCV(
    rf, param_distributions=param_dist, n_iter=50, cv=5, random_state=42)
random_search.fit(X_train, y_train)
print(f"Best parameters: {random_search.best_params_}")
print(f"Accuracy_rnd_param: {random_search.best_score_:.2f}")
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
print("Accuracy_rnd_param_on_test:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1-score:", f1_score(y_test, y_pred, average='macro'))